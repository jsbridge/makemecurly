{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Reddit images and comments\n",
    "The Pushift API is the easiest way to get data from Reddit. I used it to scrape images and comments from the r/curlyhair subreddit. These images were flaired either \"before and after\" or \"hair victory.\" I got images from as far back as images were flaired.\n",
    "\n",
    "One of the rules of the subreddit is that if you post a picture, you must post a comment describing your hair routine. Usin the same API, I grabbed all comment data associated with each image, so that I could later pull out those comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['curlyhair']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get posts and image urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://www.unixtimestamp.com/index.php\n",
    "# before and after dates\n",
    "after =  \"1464480000\" # 5/29/2016 Earliest post with flair data, that I can tell\n",
    "before = \"1600000000\"  # 9/13/2020\n",
    "\n",
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "def collectSubData(subm):\n",
    "    subData = list() # List to store the data\n",
    "    title = subm['title']  \n",
    "    author = subm['author']\n",
    "    \n",
    "    sub_id = subm['id']\n",
    "    url = subm['url']\n",
    "    subreddit = subm['subreddit']\n",
    "    created = datetime.datetime.fromtimestamp(subm['created_utc'])\n",
    "    numComms = subm['num_comments']\n",
    "\n",
    "    # If the picture doesn't have a flair, supply a NaN instead\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = np.nan\n",
    "    perm = subm['permalink']\n",
    "                 \n",
    "    subData.append((sub_id,url,perm,title,author,subreddit,created,numComms, flair))\n",
    "    subStats[sub_id] = subData\n",
    "\n",
    "# Here, I am only querying one subreddit, so this only executes one loop\n",
    "for sub in subreddits:\n",
    "    \n",
    "    print(sub)\n",
    "    query = \"\"\n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "    # Will run until all posts have been gathered \n",
    "    # from the 'after' date up until before date\n",
    "    while len(data) > 0:\n",
    "        for submission in data:\n",
    "            collectSubData(submission)\n",
    "            subCount+=1\n",
    "        after = data[-1]['created_utc']\n",
    "        try:\n",
    "            data = getPushshiftData(query, after, before, sub)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def updateSubs_file(filename):\n",
    "        upload_count = 0\n",
    "        location = \"./\"\n",
    "        file = location + filename + '.csv'\n",
    "        with open(file, 'a', newline='', encoding='utf-8') as file: \n",
    "            a = csv.writer(file, delimiter=',')\n",
    "            headers = ['sub_id','image_url','permalink','text','author','subreddit','created','n_comments','flair']\n",
    "            a.writerow(headers)\n",
    "            for sub in subStats:\n",
    "                a.writerow(subStats[sub][0])\n",
    "                upload_count+=1\n",
    "\n",
    "            print(str(upload_count) + \" submissions have been uploaded\")\n",
    "\n",
    "    updateSubs_file(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.unixtimestamp.com/index.php\n",
    "# before and after dates\n",
    "after = \"1464480000\" # 5/29/2016 Earliest post with flair data, that I can tell\n",
    "before = \"1600000000\"  # 9/13/2020\n",
    "\n",
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/comment/?size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "def collectSubData(subm):\n",
    "    subData = list() # List to store the data \n",
    "    title = subm['body']  \n",
    "    author = subm['author']\n",
    "    sub_id = subm['id']\n",
    "    parent_id = subm['parent_id']\n",
    "    subreddit = subm['subreddit']\n",
    "    created = datetime.datetime.fromtimestamp(subm['created_utc'])\n",
    "    link_id = subm['link_id']\n",
    "\n",
    "    try:\n",
    "        submitter= subm['is_submitter']\n",
    "    except KeyError:\n",
    "        submitter = np.nan\n",
    "    try:\n",
    "        perm = subm['permalink']\n",
    "    except KeyError:\n",
    "        perm = np.nan\n",
    "    \n",
    "    subData.append((sub_id,link_id,parent_id,title,author,subreddit,created,submitter, perm))\n",
    "    subStats[sub_id] = subData\n",
    "\n",
    "for sub in subreddits:\n",
    "    \n",
    "    print(sub)\n",
    "\n",
    "    query = \"\"\n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "    # Will run until all posts have been gathered \n",
    "    # from the 'after' date up until before date\n",
    "    while len(data) > 0:\n",
    "        for submission in data:\n",
    "            collectSubData(submission)\n",
    "            subCount+=1\n",
    "        after = data[-1]['created_utc']\n",
    "        data = getPushshiftData(query, after, before, sub)\n",
    "\n",
    "    def updateSubs_file(filename):\n",
    "        upload_count = 0\n",
    "        location = \"./\"\n",
    "        fil = location + 'comments_' + filename + '.csv'\n",
    "        with open(fil, 'w', newline='', encoding='utf-8') as fil: \n",
    "            a = csv.writer(fil, delimiter=',')\n",
    "            headers = ['sub_id','link_id','parent_id','text','author','subreddit','created','is_subm','permalink']\n",
    "            a.writerow(headers)\n",
    "            for sub in subStats:\n",
    "                a.writerow(subStats[sub][0])\n",
    "                upload_count+=1\n",
    "\n",
    "            print(str(upload_count) + \" submissions have been downloaded\")\n",
    "\n",
    "    updateSubs_file(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get images\n",
    "Now that I have the image URLS, I have to go grab the actual images. Note that in some cases, such as images stored on Imgur.com, rather than uploaded directly to Reddit, could not be scraped with this code. I chose to skip those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import os, io, hashlib\n",
    "\n",
    "# Define a function to grab the images\n",
    "def persist_image(folder_path:str,url:str):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        if os.path.getsize(file_path) < 5000:\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "            return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I needed a way to connect the image filenames with the URLS so that I could keep track of which image went with which comments.\n",
    "Note that this chunk of code was added later than the above code, and could only be executed once the \"curlyhair.csv\" file was created (which happends in another notebook.) That CSV file is the master file with the comments, image URLs, poster information, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curly_df = pd.read_csv('curlyhair.csv')\n",
    "\n",
    "curly_df = curly_df[curly_df['flair'] == 'hair victory']\n",
    "curly_urls = curly_df['image_url']\n",
    "\n",
    "# Create a file that has a column for the image URL and the path to the downloaded image\n",
    "f = open('hair_images/image_urls.dat', 'w')\n",
    "f.write('#           image_url                         file_path\\n')\n",
    "\n",
    "for url in curly_urls:\n",
    "    file_path = persist_image('./hair_images/', url)\n",
    "    if file_path != None:\n",
    "        f.write(f'{url}    {file_path}\\n')\n",
    "    else:\n",
    "        f.write(f'{url}    not downloaded\\n')\n",
    "f.close()\n",
    "\n",
    "# This doesn't work with imgur hosted files, and sometimes there are \"comment\" posts which \n",
    "# don't have images anyway, so big deal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
