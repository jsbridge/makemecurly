{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring natural language processing on Reddit comments\n",
    "The most important part of this notebook is the extraction of product names and types from the appropriate Reddit comment. I looked into various was of topic modeling, but because people often don't write out product names and sometimes the names are misspelled, I really had to hand-hold the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows and posts with no comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "curly_df = pd.read_csv('matched_posts.csv')\n",
    "\n",
    "# Drop duplicates and deleted comments\n",
    "curly_df.drop_duplicates(subset='sub_id', keep = False, inplace = True) \n",
    "curly_df.dropna(subset=['comm_text'], inplace=True)\n",
    "curly_df = curly_df[curly_df['comm_text'] != '[deleted]']\n",
    "curly_df = curly_df[curly_df['comm_text'] != '[removed]]']\n",
    "curly_df.index = range(len(curly_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [re.sub(r'http\\S+', '', t) for t in curly_df['comm_text']] # remove links\n",
    "text = [re.sub(r'([0-9]+?)', '', t).lower() for t in text] # remove all numbers and symbols\n",
    "text = [re.sub(r'(!|#|\\$|%|\\(|\\)|\\*|\\+|,|-|\\.|/|:|;|<|=|>|\\?|@|\\[|\\\\|\\]|\\^|_|`|{|\\||}|~)+', \"\", t)\n",
    "        for t in text]\n",
    "text = [re.sub(r\"(')+\", \"\", t) for t in text]\n",
    "curly_df['comm_text'] = [re.sub(r'\\s+\\s', ' ', t).strip() for t in text] # replace double spaces with single spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('./curly_products.csv', encoding='utf8')\n",
    "products.rename(columns={\"Unnamed: 0\": \"product\", \"Unnamed: 1\": \"type\"}, inplace=True)\n",
    "products.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], inplace=True)\n",
    "\n",
    "text = [re.sub(r'([0-1]+?)', '', t).lower() for t in products['product']] # remove all numbers and symbols\n",
    "products['product'] = [re.sub(r'(\\')+', '', t) for t in text]\n",
    "\n",
    "products['brand']=[t[0] for t in products['product'].str.split()]\n",
    "products['brand']=products['brand'].str.replace('_', ' ')\n",
    "\n",
    "text = [re.sub(r'([0-1]+?)', ' ', t).lower() for t in products['brand']] # remove all numbers and symbols\n",
    "products['brand'] = [re.sub(r\"(')+\", \"\", t) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shampoo' 'conditioner' 'leave-in' 'deep treatment' 'protein' 'cream'\n",
      " 'gel' 'mousse' 'serum' 'clarifying shampoo' 'nopoo' 'no poo' 'no-poo'\n",
      " 'lopoo' 'lo poo' 'lo-poo' 'leavein' 'leave in' 'cowash' 'co wash'\n",
      " 'co-wash' 'creme' 'lowpoo' 'low-poo' 'low poo' 'condition'\n",
      " 'deep conditioner' 'styler' 'styling' 'milk' 'mask']\n"
     ]
    }
   ],
   "source": [
    "types_of_products = products['type'].dropna().unique()\n",
    "types_of_products = np.append(types_of_products,['nopoo', 'no poo', 'no-poo', 'lopoo', 'lo poo', 'lo-poo','leavein', \n",
    "                          'leave in', 'cowash', 'co wash', 'co-wash','creme', 'lowpoo', 'low-poo', 'low poo', \n",
    "                            'condition','deep conditioner', 'styler', 'styling', 'milk', 'mask'])\n",
    "print(types_of_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curly_df['products_used'] = ''\n",
    "\n",
    "for i,text in enumerate(curly_df['comm_text']):\n",
    "    prod = []\n",
    "\n",
    "    for j,brand in enumerate(products['brand']):\n",
    "        if re.search(r'\\b'+str(brand)+r'\\b', str(text)):\n",
    "            prod.append((str(products['product'].iloc[j]), products['type'].iloc[j]))\n",
    "            \n",
    "    prod = list(dict.fromkeys(prod))\n",
    "    curly_df['products_used'].iloc[i] = prod\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shea_moisture curl and shine shampoo', 'shampoo'),\n",
       " ('shea_moisture_coconut and hibiscus curl & shine conditioner',\n",
       "  'conditioner'),\n",
       " ('shea_moisture_coconut and hibiscus curl and style milk', 'cream'),\n",
       " ('shea_moisture curl enhancing smoothie', 'cream'),\n",
       " ('cantu_moisturizing curl activator cream', 'cream'),\n",
       " ('shea_moisture_coconut & hibiscus curl & shine shampoo', 'shampoo')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curly_df['products_used'].iloc[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "curly_df.head(10)\n",
    "\n",
    "# Save the products extracted from each comment describing the poster's routine to a file\n",
    "curly_df.to_csv('dataframe_with_products.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling via LDA using spaCy and gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words not necessary for my purposes\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "curly_df['comm_text'] = curly_df['comm_text'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = curly_df['comm_text'].tolist()\n",
    "%time spacy_docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[t.lemma_.lower() for t in doc if len(t.orth_) > 2 and not t.is_stop] for doc in nlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "tokens = []\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:  # bigrams can be recognized by the \"_\" that joins the invidual words\n",
    "            docs[idx].append(token)\n",
    "            tokens.append(token)\n",
    "            \n",
    "print(list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in original documents:', len(dictionary))\n",
    "\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.25)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))\n",
    "\n",
    "print(\"Example representation of document 3:\", dictionary.doc2bow(docs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, chunksize=500, passes=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (topic, words) in model.print_topics():\n",
    "    print(topic+1, \":\", words, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
